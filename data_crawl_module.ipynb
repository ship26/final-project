{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72fa8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import schedule\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pytz import timezone\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta,datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878b0cd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20100/1498528994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpress_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'한국경제'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'015'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'매일경제'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'009'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'조선일보'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'023'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'동아일보'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'020'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'한겨레신문'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'028'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'경향신문'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'032'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpress_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpress_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpress_codes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpress_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20100/1498528994.py\u001b[0m in \u001b[0;36mCrawler\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\[.*\\]|\\s-\\s.*\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mnow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Asia/Seoul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mstr_now\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mformatting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnow_yymmdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr_now\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "class Crawler:   \n",
    "    press_list = {'한국경제':'015', '매일경제':'009', '조선일보':'023', '동아일보':'020', '한겨레신문':'028', '경향신문':'032'}\n",
    "    press_names = list(press_list.keys())\n",
    "    press_codes = list(press_list.values())\n",
    "\n",
    "    formatting = '%Y-%m-%d'\n",
    "    condition = \"\\[.*\\]|\\s-\\s.*\" \n",
    "\n",
    "    now = datetime.now(timezone('Asia/Seoul'))\n",
    "    str_now = datetime.strftime(now,formatting)\n",
    "    now_yymmdd = str_now.split('-')\n",
    "    now_yymmdd = [int (i) for i in now_yymmdd]\n",
    "\n",
    "    yesterday = now - timedelta(1)\n",
    "    str_yesterday = datetime.strftime(yesterday,formatting)\n",
    "    yesterday_yymmdd = str_yesterday.split('-')\n",
    "    yesterday_yymmdd = [int(i) for i in yesterday_yymmdd] \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def crawling_yesterday(self):\n",
    "        # 크롤링 할 대상 입력\n",
    "        for press in self.press_names:\n",
    "            year, month, day = map(int, self.yesterday_yymmdd[0:3])\n",
    "            to_crawl_press_this_time = press\n",
    "            presscode = self.press_list[to_crawl_press_this_time]\n",
    "\n",
    "            # 6개 신문사 불러오기 \n",
    "            df = pd.DataFrame(columns=[\"press\",\"title\",\"date\"])\n",
    "            yesterday_data = []\n",
    "            if int(month) < 10:\n",
    "                month = f\"0{month}\"\n",
    "            print(f'{to_crawl_press_this_time}의 {month}월 {day}일 기사를 크롤링합니다.')\n",
    "            print(\"**************************************\")\n",
    "\n",
    "            if day in tqdm(range(1,32)):\n",
    "                # URL에 접속 시도, 1 ~ 9일에 대한 URL 접근\n",
    "                if day < 10:\n",
    "                    URL = f'https://media.naver.com/press/{presscode}/newspaper?date={year}{month}0{day}'\n",
    "                    print(URL)\n",
    "                    response = requests.get(URL)\n",
    "                    print(response)\n",
    "\n",
    "                # URL에 접속 시도,10일부터에 대한 URL 접근\n",
    "                else:\n",
    "                    URL = f'https://media.naver.com/press/{presscode}/newspaper?date={year}{month}{day}'\n",
    "                    print(URL)\n",
    "                    response = requests.get(URL)\n",
    "                    print(response)\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "                # html 파싱 시작\n",
    "                soup = BeautifulSoup(response.text)\n",
    "                page = soup.select('div.newspaper_inner')\n",
    "\n",
    "                # 혹시나 1면도 없으면 신문 없는 날\n",
    "                if len(page) < 1:\n",
    "                    print(f'{day}일자는 기사가 없습니다 데헷')\n",
    "                    print('-' * 25)\n",
    "                    continue\n",
    "\n",
    "                # 신문 지면 발견하면 크롤링\n",
    "                else:\n",
    "                    print(f'{day}일자 기사를 크롤링합니다.')\n",
    "                    print('-' * 25)\n",
    "\n",
    "                    # 하루치 기사를 담을 리스트\n",
    "                    oneday_titles = []\n",
    "\n",
    "                    # 지면별 헤드라인 탐색\n",
    "                    for _ in range(len(page)):\n",
    "                        page_titles = []\n",
    "                        title = soup.select('div.newspaper_inner')[_]\n",
    "                        titles = title.find_all('strong')\n",
    "                        urls=title.find_all('a')\n",
    "                        href_list=[]\n",
    "                        for a in urls:\n",
    "                            href = a.attrs['href']\n",
    "                            href_list.append(href)\n",
    "\n",
    "                        # 헤드라인들 page_title 리스트에 담음\n",
    "                        for ttl in range(len(titles)):\n",
    "                            news_title = titles[ttl].text\n",
    "                            url = href_list[ttl]\n",
    "\n",
    "                            \n",
    "                            if len(news_title) < 8:\n",
    "                                news_title == np.nan\n",
    "                            if day < 10:\n",
    "                                news = {'press':to_crawl_press_this_time,'title':news_title, 'date':f'{year}{month}0{day}','url':url}\n",
    "                            else:\n",
    "                                news = {'press':to_crawl_press_this_time,'title':news_title, 'date':f'{year}{month}{day}','url':url}\n",
    "\n",
    "                            yesterday_data.append(news)\n",
    "\n",
    "            df = df.append(yesterday_data)\n",
    "            df = df.dropna(axis = 0)\n",
    "            df.to_csv(f'data/common/{to_crawl_press_this_time}_{year}_{month}_{day}.csv', encoding=\"UTF-8-sig\")\n",
    "        return print(\"어제자 일반뉴스 크롤링 및 저장 완료\")\n",
    "\n",
    "    \n",
    "    def crawling_today(self):\n",
    "        # 크롤링 할 대상 입력\n",
    "        for press in self.press_names:\n",
    "            year, month, day = map(int, self.now_yymmdd[0:3])\n",
    "            to_crawl_press_this_time = press\n",
    "            presscode = self.press_list[to_crawl_press_this_time]\n",
    "\n",
    "            # 6개 신문사 불러오기 \n",
    "            df = pd.DataFrame(columns=[\"press\",\"title\",\"date\"])\n",
    "            today_data = []\n",
    "\n",
    "            if month < 10:\n",
    "                month = f\"0{month}\"\n",
    "            print(f'{to_crawl_press_this_time}의 {month}월 {day}일 기사를 크롤링합니다.')\n",
    "            print(\"**************************************\")\n",
    "\n",
    "            if day in tqdm(range(1,32)):\n",
    "                # URL에 접속 시도,1 ~ 9일에 대한 URL 접근\n",
    "                if day < 10:\n",
    "                    URL = f'https://media.naver.com/press/{presscode}/newspaper?date={year}{month}0{day}'\n",
    "                    print(URL)\n",
    "                    response = requests.get(URL)\n",
    "                    print(response)\n",
    "                # 10일부터에 대한 URL 접근\n",
    "                else:\n",
    "                    URL = f'https://media.naver.com/press/{presscode}/newspaper?date={year}{month}{day}'\n",
    "                    print(URL)\n",
    "                    response = requests.get(URL)\n",
    "                    print(response)\n",
    "                    \n",
    "                time.sleep(1)\n",
    "                \n",
    "                # html 파싱 시작\n",
    "                soup = BeautifulSoup(response.text)\n",
    "                page = soup.select('div.newspaper_inner')\n",
    "\n",
    "                # 혹시나 1면도 없으면 신문 없는 날\n",
    "                if len(page) < 1:\n",
    "                    print(f'{day}일자는 기사가 없습니다 데헷')\n",
    "                    print('-' * 25)\n",
    "                    continue\n",
    "\n",
    "                # 신문 지면 발견하면 크롤링\n",
    "                else:\n",
    "                    print(f'{day}일자 기사를 크롤링합니다.')\n",
    "                    print('-' * 25)\n",
    "\n",
    "                    # 하루치 기사를 담을 리스트\n",
    "                    oneday_titles = []\n",
    "\n",
    "                    # 지면별 헤드라인 탐색\n",
    "                    for _ in range(len(page)):\n",
    "                        page_titles = []\n",
    "                        title = soup.select('div.newspaper_inner')[_]\n",
    "                        titles = title.find_all('strong')\n",
    "                        urls=title.find_all('a')\n",
    "                        href_list=[]\n",
    "                        for a in urls:\n",
    "                            href = a.attrs['href']\n",
    "                            href_list.append(href)\n",
    "\n",
    "                        # 헤드라인들 page_title 리스트에 담음\n",
    "                        for ttl in range(len(titles)):\n",
    "                            news_title = titles[ttl].text\n",
    "                            url = href_list[ttl]\n",
    "                            if len(news_title) < 8:\n",
    "                                news_title == np.nan\n",
    "                            if day < 10:\n",
    "                                news = {'press':to_crawl_press_this_time,'title':news_title, 'date':f'{year}{month}0{day}','url':url}\n",
    "                            else:\n",
    "                                news = {'press':to_crawl_press_this_time,'title':news_title, 'date':f'{year}{month}{day}','url':url}\n",
    "\n",
    "                            today_data.append(news)\n",
    "\n",
    "            df = df.append(today_data)\n",
    "            df = df.dropna(axis = 0)\n",
    "            if day < 10:\n",
    "                df.to_csv(f'data/common/{to_crawl_press_this_time}_{year}_{month}_0{day}.csv', encoding=\"UTF-8-sig\")\n",
    "            else:\n",
    "                df.to_csv(f'data/common/{to_crawl_press_this_time}_{year}_{month}_{day}.csv', encoding=\"UTF-8-sig\")\n",
    "        \n",
    "        return print(\"오늘자 일반뉴스 크롤링 및 저장 완료.\")\n",
    "\n",
    "    \n",
    "    def crawling_yesterday_finance(self):\n",
    "        # 크롤링 할 대상 입력\n",
    "        yesterday_date = self.str_yesterday\n",
    "        \n",
    "        df = pd.DataFrame(columns=[\"press\",\"title\",\"date\"])\n",
    "        yesterday_data = []  #데이터 담을 빈 리스트\n",
    "        print(f'네이버금융뉴스의 {yesterday_date}자 기사를 크롤링합니다.')\n",
    "        print(\"**************************************\")\n",
    "\n",
    "        url = f'https://finance.naver.com/news/mainnews.naver?date={yesterday_date}'\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        # html 파싱 시작\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        titles = soup.find_all('li', class_='block1')\n",
    "        url_plus='https://finance.naver.com/'\n",
    "\n",
    "        if titles:\n",
    "            for i in titles:\n",
    "                temp=i.find_all('a')\n",
    "                #썸네일있는 기사와 없는기사 구분해서 크롤링\n",
    "                if len(temp)==2:\n",
    "                    title=temp[1].text\n",
    "                    url=temp[1].attrs['href']\n",
    "                    text = re.sub(self.condition, '',title)\n",
    "                    if len(text) < 8:\n",
    "                        text == np.nan\n",
    "                    news = {'press':'네이버금융','title':text, 'date':yesterday_date,'url':url_plus+url}\n",
    "                    yesterday_data.append(news)\n",
    "                else:\n",
    "                    title=temp[0].text\n",
    "                    url=temp[0].attrs['href']\n",
    "                    text = re.sub(self.condition, '',title)\n",
    "                    if len(text) < 8:\n",
    "                        text == np.nan\n",
    "                    news = {'press':'네이버금융','title':text, 'date':yesterday_date,'url':url_plus+url}\n",
    "                    yesterday_data.append(news)\n",
    "        else:\n",
    "            print('기사가 없습니다.')\n",
    "\n",
    "        df = df.append(yesterday_data)\n",
    "        df = df.dropna(axis = 0)\n",
    "        df.to_csv(f'data/finance/finance_{yesterday_date}.csv', encoding=\"UTF-8-sig\")\n",
    "        return print(\"어제 자 금융뉴스 크롤링 및 저장 완료\")\n",
    "\n",
    "    \n",
    "    def crawling_today_finance(self):\n",
    "        # 크롤링 할 대상 입력\n",
    "        today_date = self.str_now\n",
    "        \n",
    "        df = pd.DataFrame(columns=[\"press\",\"title\",\"date\"])\n",
    "        today_data = []  #데이터 담을 빈 리스트\n",
    "        print(f'네이버금융뉴스의 {today_date}날짜 기사를 크롤링합니다.')\n",
    "        print(\"**************************************\")\n",
    "\n",
    "\n",
    "        url = f'https://finance.naver.com/news/mainnews.naver?date={today_date}'\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "        # html 파싱 시작\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        titles = soup.find_all('li', class_='block1')\n",
    "        url_plus='https://finance.naver.com/'\n",
    "\n",
    "        if titles:\n",
    "            for i in titles:\n",
    "                temp=i.find_all('a')\n",
    "                #썸네일있는 기사와 없는기사 구분해서 크롤링\n",
    "                if len(temp)==2:\n",
    "                    title=temp[1].text\n",
    "                    url=temp[1].attrs['href']\n",
    "                    text = re.sub(self.condition, '',title)\n",
    "                    if len(text) < 8:\n",
    "                        text == np.nan\n",
    "                    news = {'press':'네이버금융','title':text, 'date':today_date,'url':url_plus+url}\n",
    "                    today_data.append(news)\n",
    "                else:\n",
    "                    title=temp[0].text\n",
    "                    url=temp[0].attrs['href']\n",
    "                    text = re.sub(self.condition, '',title)\n",
    "                    if len(text) < 8:\n",
    "                        text == np.nan\n",
    "                    news = {'press':'네이버금융','title':text, 'date':today_date,'url':url_plus+url}\n",
    "                    today_data.append(news)\n",
    "        else:\n",
    "            print('기사가 없습니다.')\n",
    "\n",
    "        df = df.append(today_data)\n",
    "        df = df.dropna(axis = 0)\n",
    "        df.to_csv(f'data/finance/finance_{today_date}.csv', encoding=\"UTF-8-sig\")\n",
    "        \n",
    "        return print(\"오늘 자 금융뉴스 크롤링 및 저장 완료\")\n",
    "    \n",
    "            \n",
    "    def merge(self):\n",
    "        \n",
    "        df_all = pd.DataFrame()\n",
    "        path_of_this_file = os.getcwd()\n",
    "        \n",
    "        common_folder = os.listdir(f\"{path_of_this_file}/data/common\")# 파일들이 있는 폴더명으로 폴더내 파일 목록 확인\n",
    "        common_folder.remove('daily')\n",
    "\n",
    "        \n",
    "        # 파일 병합 및 저장 \n",
    "        for file in common_folder:\n",
    "            file = f'{path_of_this_file}/data/common/{file}'\n",
    "            df = pd.read_csv(file, encoding='utf-8-sig', index_col=0)\n",
    "            df_all = pd.concat([df_all, df])\n",
    "               \n",
    "        df_all.dropna()\n",
    "        df_all.to_csv(f'{path_of_this_file}/data/common/daily/{self.str_now}.csv', encoding=\"utf-8-sig\")   \n",
    "        \n",
    "        # 기존파일 삭제\n",
    "        for file in common_folder:\n",
    "            file = f'{path_of_this_file}/data/common/{file}'\n",
    "            os.remove(file)\n",
    "        \n",
    "        return print(\"어제와 오늘 일반뉴스 파일 병합 및 저장 완료.\")\n",
    "\n",
    "    \n",
    "    def merge_finance(self):\n",
    "        \n",
    "        df_all = pd.DataFrame()\n",
    "        path_of_this_file = os.getcwd()\n",
    "        \n",
    "        finance_folder = os.listdir(f\"{path_of_this_file}/data/finance\")# 파일들이 있는 폴더명으로 폴더내 파일 목록 확인\n",
    "        finance_folder.remove('daily')\n",
    "        \n",
    "        # 파일 병합 및 저장\n",
    "        for file in finance_folder:\n",
    "            file = f'{path_of_this_file}/data/finance/{file}'\n",
    "            df = pd.read_csv(file, encoding='utf-8-sig', index_col=0)\n",
    "            df_all = pd.concat([df_all, df])\n",
    "        \n",
    "        df_all.to_csv(f'{path_of_this_file}/data/finance/daily/{self.str_now}.csv', encoding=\"utf-8-sig\")   \n",
    "        \n",
    "        # 기존파일 삭제\n",
    "        for file in finance_folder:\n",
    "            file = f'{path_of_this_file}/data/finance/{file}'\n",
    "            os.remove(file)\n",
    "        \n",
    "        return print(\"어제와 오늘 금융뉴스 파일 병합 및 저장 완료.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f22f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
