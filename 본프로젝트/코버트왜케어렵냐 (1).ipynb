{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910a0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy<1.17.0,>=1.8.2 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from mxnet) (1.16.6)\n",
      "Requirement already satisfied: requests<2.19.0,>=2.18.4 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.20\n",
      "    Uninstalling graphviz-0.20:\n",
      "      Successfully uninstalled graphviz-0.20\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.7.0.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d751383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonnlp in c:\\users\\ship2\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ship2\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ship2\\anaconda3\\lib\\site-packages (4.62.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from gluonnlp) (21.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from gluonnlp) (1.16.6)\n",
      "Requirement already satisfied: cython in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from gluonnlp) (0.29.28)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting numpy>=1.16.0\n",
      "  Downloading numpy-1.23.1-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     --------------------------------------- 14.7/14.7 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from packaging->gluonnlp) (3.0.4)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.6\n",
      "    Uninstalling numpy-1.16.6:\n",
      "      Successfully uninstalled numpy-1.16.6\n",
      "Successfully installed numpy-1.23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "tensorflow 2.5.3 requires numpy~=1.19.2, but you have numpy 1.23.1 which is incompatible.\n",
      "tensorflow 2.5.3 requires typing-extensions~=3.7.4, but you have typing-extensions 4.3.0 which is incompatible.\n",
      "tensorflow-datasets 4.6.0 requires requests>=2.19.0, but you have requests 2.18.4 which is incompatible.\n",
      "tensorboard 2.8.0 requires requests<3,>=2.21.0, but you have requests 2.18.4 which is incompatible.\n",
      "scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.1 which is incompatible.\n",
      "pandas-datareader 0.10.0 requires requests>=2.19.0, but you have requests 2.18.4 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.23.1 which is incompatible.\n",
      "mxnet 1.7.0.post2 requires numpy<1.17.0,>=1.8.2, but you have numpy 1.23.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ship2\\anaconda3\\lib\\site-packages (0.1.96)\n",
      "Collecting transformers==3.0.2\n",
      "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.1.96)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (4.62.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2.18.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2021.8.3)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (3.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (1.23.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (21.0)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Using cached tokenizers-0.8.1rc1.tar.gz (97 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==3.0.2) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from packaging->transformers==3.0.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (8.0.3)\n",
      "Requirement already satisfied: six in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [48 lines of output]\n",
      "      C:\\Users\\ship2\\AppData\\Local\\Temp\\pip-build-env-dbdwnm8b\\overlay\\Lib\\site-packages\\setuptools\\dist.py:530: UserWarning: Normalizing '0.8.1.rc1' to '0.8.1rc1'\n",
      "        warnings.warn(tmpl.format(**locals()))\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ship2\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ship2\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet # 코랩 환경이기 때문에 앞에 !를 붙여야 한다.\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3.0.2\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54477178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a9f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b56dff9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kobert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27976/296950403.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#kobert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkobert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkobert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpytorch_kobert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_pytorch_kobert_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#transformers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kobert'"
     ]
    }
   ],
   "source": [
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW # 인공지능 모델의 초기값 지정 함수를 아담으로 지정한다.\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9527364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n_devices = torch.cuda.device_count()\n",
    "print(n_devices)\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9ed54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4db3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/lab09/.cache/kobert_v1.zip\n",
      "using cached model. /home/lab09/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c725fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "naturalTraining_data = pd.read_csv('테스트.csv',encoding='utf-8',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d1ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emote_change(x):\n",
    "    if x==0:\n",
    "        x='부정'\n",
    "    elif x==1:\n",
    "        x='긍정'\n",
    "    else:\n",
    "        x='중립'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a2d150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                코로나 이겨내고 엄마는 ‘희망’을 낳았다\n",
       "1                                  靑비서실장 유영민, 민정수석에 신현수\n",
       "2         이재명 18.2%, 이낙연 16.2%, 윤석열 15.1%..오차범위 내 3강 구도\n",
       "3                            난 그래도 낙관한다, 눈에도 바람에도 지지않기를\n",
       "4                  유영민, 두달 전 비서실장 되나 묻자 “풀 뜯고 사는데 무슨...\n",
       "5     노영민 “대통령 제대로 보필 못해 죄송”', '김상조 사의 반려한 文대통령 “현안 ...\n",
       "6                      與후보 적합도, 이재명·이낙연 順… 野선 윤석열·홍준표 順\n",
       "7                      野후보 찍어 정권교체” 49.9%, “與후보 뽑아 정권유지\n",
       "8               4개월 의식불명 끝에 깨어난 美간호사…', '아가들아, 버텨줘서 고마워\n",
       "9                      코로나 사망 하루 1000명… ‘방역 우등생’ 독일의 추락\n",
       "10                           중국 “美, 홍콩부동산 팔려면 우리 동의 받아라\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=naturalTraining_data\n",
    "data1['emote']=data1['emote'].apply(emote_change)\n",
    "data1.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9f5833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['코로나 이겨내고 엄마는 ‘희망’을 낳았다', '0'],\n",
       " ['靑비서실장 유영민, 민정수석에 신현수', '2'],\n",
       " ['이재명 18.2%, 이낙연 16.2%, 윤석열 15.1%..오차범위 내 3강 구도', '2'],\n",
       " ['난 그래도 낙관한다, 눈에도 바람에도 지지않기를', '0'],\n",
       " ['유영민, 두달 전 비서실장 되나 묻자 “풀 뜯고 사는데 무슨...', '2'],\n",
       " [\"노영민 “대통령 제대로 보필 못해 죄송”', '김상조 사의 반려한 文대통령 “현안 많아 교체할 때 아니다\", '2'],\n",
       " ['與후보 적합도, 이재명·이낙연 順… 野선 윤석열·홍준표 順', '2'],\n",
       " ['野후보 찍어 정권교체” 49.9%, “與후보 뽑아 정권유지', '2'],\n",
       " [\"4개월 의식불명 끝에 깨어난 美간호사…', '아가들아, 버텨줘서 고마워\", '0'],\n",
       " ['코로나 사망 하루 1000명… ‘방역 우등생’ 독일의 추락', '1'],\n",
       " ['중국 “美, 홍콩부동산 팔려면 우리 동의 받아라', '1']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data1['emote'])\n",
    "data1['emote'] = encoder.transform(data1['emote'])\n",
    "data1.head(10)\n",
    "\n",
    "데이터리스트=[]\n",
    "for 타이틀, 감정 in zip(data1['title'], data1['emote'])  :\n",
    "    data = []\n",
    "    data.append(타이틀)\n",
    "    data.append(str(감정))\n",
    "\n",
    "    데이터리스트.append(data)\n",
    "데이터리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5640d3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '긍정', 1: '부정', 2: '중립'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = dict(zip(range(len(encoder.classes_)), encoder.classes_))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3999536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: 8\n",
      "test shape is: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(데이터리스트, test_size=0.2, random_state=42) # new_df 오탈자 수정\n",
    "print(\"train shape is:\", len(train))\n",
    "print(\"test shape is:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ad75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/lab09/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6e29e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['중국 “美, 홍콩부동산 팔려면 우리 동의 받아라', '1'],\n",
       " ['이재명 18.2%, 이낙연 16.2%, 윤석열 15.1%..오차범위 내 3강 구도', '2'],\n",
       " ['靑비서실장 유영민, 민정수석에 신현수', '2'],\n",
       " [\"4개월 의식불명 끝에 깨어난 美간호사…', '아가들아, 버텨줘서 고마워\", '0'],\n",
       " ['유영민, 두달 전 비서실장 되나 묻자 “풀 뜯고 사는데 무슨...', '2'],\n",
       " ['野후보 찍어 정권교체” 49.9%, “與후보 뽑아 정권유지', '2'],\n",
       " ['난 그래도 낙관한다, 눈에도 바람에도 지지않기를', '0'],\n",
       " ['與후보 적합도, 이재명·이낙연 順… 野선 윤석열·홍준표 順', '2']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99baaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노영민 “대통령 제대로 보필 못해 죄송”', '김상조 사의 반려한 文대통령 “현안 많아 교체할 때 아니다\n",
      "코로나 이겨내고 엄마는 ‘희망’을 낳았다\n",
      "코로나 사망 하루 1000명… ‘방역 우등생’ 독일의 추락\n"
     ]
    }
   ],
   "source": [
    "transform = nlp.data.BERTSentenceTransform(\n",
    "            tok, max_seq_length=64, pad=True, pair=False) \n",
    "for i in range(len(test)):\n",
    "    print(test.iloc[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a51f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddecebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e19271ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "559779b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# pytorch용 DataLoader 사용\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e45a38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 3, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f589232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "059f4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bed799fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 선언\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d19001d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10761/1806958373.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08aaaecc74a46c6bb1997e83c30f975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.0380128622055054 train acc 0.5\n",
      "epoch 1 train acc 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10761/1806958373.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacde168d86f4763b3bf84fa88091870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06be8767e97d48d583375189ff267b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.8708484172821045 train acc 0.625\n",
      "epoch 2 train acc 0.625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4b74a9df26400c9856e4af9df7030a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b209b6ac0c49a6a0592f00e3c59845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.8898464441299438 train acc 0.75\n",
      "epoch 3 train acc 0.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8ba2f94c264761a2d25dfbce3b33ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7351b113c74e3e94ed5dae5700d094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.7557373642921448 train acc 0.75\n",
      "epoch 4 train acc 0.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13beba8e3a2479eade480869046a602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb82911ec20497d9cfb2cc9af7a1910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.9281213283538818 train acc 0.5\n",
      "epoch 5 train acc 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57a28c4342c40f8bb0395ff003a0c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval() # 평가 모드로 변경\n",
    "    \n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00f6be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10761/1121450162.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f159d8687154c40b62d8dc208a5a97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중립\n"
     ]
    }
   ],
   "source": [
    "# 테스트 문장 예측\n",
    "test_sentence = '낙관한다.'\n",
    "test_label = 0 # 감성 라벨\n",
    "\n",
    "unseen_test = pd.DataFrame([[test_sentence, test_label]], columns = [['뉴스 타이틀', '감성']])\n",
    "unseen_values = unseen_test.values\n",
    "\n",
    "test_set = BERTDataset(unseen_values, 0, 1, tok, max_len, True, False)\n",
    "test_input = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=5)\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_input)):\n",
    "  token_ids = token_ids.long().to(device)\n",
    "  segment_ids = segment_ids.long().to(device)\n",
    "  valid_length= valid_length\n",
    "  out = model(token_ids, valid_length, segment_ids)\n",
    "    \n",
    "test_eval=[]\n",
    "for i in out:\n",
    "        logits=i\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        if np.argmax(logits) == 0:\n",
    "            test_eval.append(\"부정\")\n",
    "        elif np.argmax(logits) == 1:\n",
    "            test_eval.append(\"긍정\")\n",
    "        elif np.argmax(logits) == 2:\n",
    "            test_eval.append(\"중립\")\n",
    "\n",
    "print(test_eval[0])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c792c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
